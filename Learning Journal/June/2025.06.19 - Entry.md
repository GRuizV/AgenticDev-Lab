# 📘 Roo Learning Log – June 19, 2025

## 🧑‍💻 Session Overview
**Mode:** Evaluation Logging / PoC Expansion  
**Project:** Personal Billing Assistant (PBA)  
**Focus:** Recording and analyzing results of Test #0 — manual LLM evaluation

---

## ✅ Summary of Work

### 🧪 Test #0 Recorded
- Ran 3 LLMs (Claude, ChatGPT, Gemini) on 6 structured PDF credit card statements
- Used identical prompts across models
- Captured and validated results manually against **ground truth** of 57 transactions totaling ~$5.1M COP

### 📈 Evaluation Metrics
- Used ±$5 tolerance for matching
- Measured **Precision, Recall, and F1** for each model
- **Claude Sonnet 4** was most accurate overall
- **Gemini** had structural issues with tables and alignment

### 🧠 Key Observations
- Prompt design and input formatting greatly affected output quality
- Some models hallucinated fields or ignored clear entries
- Model behavior varied across file types and layout complexity

---

## 📁 Artifacts Logged
- `project_summary_report.md`: Full write-up of methodology, results, and recommendations
- Result JSONs (referenced, not uploaded here)

---

## 🔜 Next Steps
- Move from manual prompting to **API-driven evaluation**
- Design fallback logic and robustness prompts
- Introduce controlled corruption or OCR errors for error tolerance testing
- Expand evaluation pipeline into reproducible test suite

---

📌 **Status:** Test #0 Completed — baseline performance captured, ready to automate